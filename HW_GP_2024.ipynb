{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQEHAFhU0xzx"
   },
   "source": [
    "# Regression with Gaussian Processes\n",
    "\n",
    "------------------------------------------------------\n",
    "*Machine Learning, Master in Big Data Analytics, 2023-2024*\n",
    "\n",
    "*Pablo M. Olmos olmos@tsc.uc3m.es, Emilio Parrado Hernandez, eparrado@ing.uc3m.es*\n",
    "\n",
    "------------------------------------------------------\n",
    "\n",
    "The aim of this homework is to solve a real data problem using Gaussian Processes.\n",
    "\n",
    "The problem is the prediction of both the heating load (HL) and cooling load (CL) of residential buildings. We consider eight input variables for each building: relative compactness, surface area, wall area, roof area, overall height, orientation, glazing area, glazing area distribution.\n",
    "\n",
    "In this [paper](https://www.sciencedirect.com/science/article/pii/S037877881200151X) you can find a detailed description of the problem and a solution based on linear regression [(iteratively reweighted least squares (IRLS) algorithm)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&ved=2ahUKEwjZuoLY2OjgAhUs3uAKHUZ7BVcQFjAJegQIAhAC&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F9b92%2F18e7233f4d0b491e1582c893c9a099470a73.pdf&usg=AOvVaw3YDwqZh1xyF626VqfnCM2k) and random forests. Using GPs, our goal is not only estimate accurately both HL and CL, but also get a measure of uncertainty in our predictions.\n",
    "\n",
    "The data set can be downloaded from the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9XScdSK0xz3"
   },
   "source": [
    "## 1. Loading and preparing the data\n",
    "\n",
    "* Download the dataset\n",
    "* Divide at random the dataset into train (80%) and test (20%) datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lvFIbLzT0xz3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative Compactness</th>\n",
       "      <th>Surface Area</th>\n",
       "      <th>Wall Area</th>\n",
       "      <th>Roof Area</th>\n",
       "      <th>Overall Height</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>Glazing Area</th>\n",
       "      <th>Glazing Area Distribution</th>\n",
       "      <th>Heating Load</th>\n",
       "      <th>Cooling Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.44</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.64</td>\n",
       "      <td>16.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Relative Compactness  Surface Area  Wall Area  Roof Area  Overall Height  \\\n",
       "0                    0.98         514.5      294.0     110.25             7.0   \n",
       "1                    0.98         514.5      294.0     110.25             7.0   \n",
       "2                    0.98         514.5      294.0     110.25             7.0   \n",
       "3                    0.98         514.5      294.0     110.25             7.0   \n",
       "4                    0.90         563.5      318.5     122.50             7.0   \n",
       "..                    ...           ...        ...        ...             ...   \n",
       "763                  0.64         784.0      343.0     220.50             3.5   \n",
       "764                  0.62         808.5      367.5     220.50             3.5   \n",
       "765                  0.62         808.5      367.5     220.50             3.5   \n",
       "766                  0.62         808.5      367.5     220.50             3.5   \n",
       "767                  0.62         808.5      367.5     220.50             3.5   \n",
       "\n",
       "     Orientation  Glazing Area  Glazing Area Distribution  Heating Load  \\\n",
       "0              2           0.0                          0         15.55   \n",
       "1              3           0.0                          0         15.55   \n",
       "2              4           0.0                          0         15.55   \n",
       "3              5           0.0                          0         15.55   \n",
       "4              2           0.0                          0         20.84   \n",
       "..           ...           ...                        ...           ...   \n",
       "763            5           0.4                          5         17.88   \n",
       "764            2           0.4                          5         16.54   \n",
       "765            3           0.4                          5         16.44   \n",
       "766            4           0.4                          5         16.48   \n",
       "767            5           0.4                          5         16.64   \n",
       "\n",
       "     Cooling Load  \n",
       "0           21.33  \n",
       "1           21.33  \n",
       "2           21.33  \n",
       "3           21.33  \n",
       "4           28.28  \n",
       "..            ...  \n",
       "763         21.40  \n",
       "764         16.88  \n",
       "765         17.11  \n",
       "766         16.61  \n",
       "767         16.03  \n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "data = pd.read_excel('ENB2012_data.xlsx')\n",
    "\n",
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'X1': 'Relative Compactness',\n",
    "    'X2': 'Surface Area',\n",
    "    'X3': 'Wall Area',\n",
    "    'X4': 'Roof Area',\n",
    "    'X5': 'Overall Height',\n",
    "    'X6': 'Orientation',\n",
    "    'X7': 'Glazing Area',\n",
    "    'X8': 'Glazing Area Distribution',\n",
    "    'Y1': 'Heating Load',\n",
    "    'Y2': 'Cooling Load'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "data = data.rename(columns=new_column_names)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline results: Random Forests (10%)\n",
    "\n",
    "Train a Random Forests selecting the number of trees in the forest and the maximum number of leaves using cross validation. \n",
    "\n",
    "**Print the scores in the test set for each target. These will be our baseline results.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load: 0.4923581414788641\n",
      "RMSE for Cooling Load: 1.7456331110077932\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "# Separate features and target variables\n",
    "X = data.drop(columns=['Heating Load', 'Cooling Load'])\n",
    "y_heating = data['Heating Load']\n",
    "y_cooling = data['Cooling Load']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train_heating, y_test_heating = train_test_split(X, y_heating, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_cooling, y_test_cooling = train_test_split(X, y_cooling, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define preprocessing steps for numerical features\n",
    "numeric_features = X.columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Define the Random Forest Regressor pipeline\n",
    "rf_pipeline_heating = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "rf_pipeline_cooling = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [50, 100, 150],\n",
    "    'regressor__max_leaf_nodes': [None, 10, 20, 30]\n",
    "}\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters for heating load\n",
    "grid_search_heating = RandomizedSearchCV(rf_pipeline_heating, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_heating.fit(X_train, y_train_heating)\n",
    "\n",
    "# Perform cross-validation to find the best hyperparameters for cooling load\n",
    "grid_search_cooling = RandomizedSearchCV(rf_pipeline_cooling, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_cooling.fit(X_train, y_train_cooling)\n",
    "\n",
    "# Get the best estimators\n",
    "best_rf_heating = grid_search_heating.best_estimator_\n",
    "best_rf_cooling = grid_search_cooling.best_estimator_\n",
    "\n",
    "# Fit the models on the training data\n",
    "best_rf_heating.fit(X_train, y_train_heating)\n",
    "best_rf_cooling.fit(X_train, y_train_cooling)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_heating = best_rf_heating.predict(X_test)\n",
    "y_pred_cooling = best_rf_cooling.predict(X_test)\n",
    "\n",
    "# Calculate RMSE for each target variable\n",
    "rmse_heating_RF = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "rmse_cooling_RF = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load:\", rmse_heating_RF)\n",
    "print(\"RMSE for Cooling Load:\", rmse_cooling_RF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use attribute `feature_importances_` to discuss which are, for RF, the most informative features to predict each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance for Heating Load:\n",
      "                     Feature  Importance for Heating Load  \\\n",
      "0       Relative Compactness                     0.292188   \n",
      "1               Surface Area                     0.256931   \n",
      "3                  Roof Area                     0.191848   \n",
      "4             Overall Height                     0.126243   \n",
      "6               Glazing Area                     0.079281   \n",
      "2                  Wall Area                     0.040570   \n",
      "7  Glazing Area Distribution                     0.012163   \n",
      "5                Orientation                     0.000776   \n",
      "\n",
      "   Importance for Cooling Load  \n",
      "0                     0.474130  \n",
      "1                     0.115219  \n",
      "3                     0.019252  \n",
      "4                     0.272846  \n",
      "6                     0.048612  \n",
      "2                     0.042392  \n",
      "7                     0.015397  \n",
      "5                     0.012153  \n",
      "\n",
      "Feature importance for Cooling Load:\n",
      "                     Feature  Importance for Heating Load  \\\n",
      "0       Relative Compactness                     0.292188   \n",
      "4             Overall Height                     0.126243   \n",
      "1               Surface Area                     0.256931   \n",
      "6               Glazing Area                     0.079281   \n",
      "2                  Wall Area                     0.040570   \n",
      "3                  Roof Area                     0.191848   \n",
      "7  Glazing Area Distribution                     0.012163   \n",
      "5                Orientation                     0.000776   \n",
      "\n",
      "   Importance for Cooling Load  \n",
      "0                     0.474130  \n",
      "4                     0.272846  \n",
      "1                     0.115219  \n",
      "6                     0.048612  \n",
      "2                     0.042392  \n",
      "3                     0.019252  \n",
      "7                     0.015397  \n",
      "5                     0.012153  \n"
     ]
    }
   ],
   "source": [
    "# Get feature importances for heating load prediction\n",
    "feature_importance_heating = best_rf_heating.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Get feature importances for cooling load prediction\n",
    "feature_importance_cooling = best_rf_cooling.named_steps['regressor'].feature_importances_\n",
    "\n",
    "# Combine feature importances with feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance for Heating Load': feature_importance_heating,\n",
    "    'Importance for Cooling Load': feature_importance_cooling\n",
    "})\n",
    "\n",
    "# Sort features by importance for each target\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance for Heating Load', ascending=False)\n",
    "print(\"Feature importance for Heating Load:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance for Cooling Load', ascending=False)\n",
    "print(\"\\nFeature importance for Cooling Load:\")\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WtDRWaX0xz4"
   },
   "source": [
    "## 3. First result Gaussian Process (10%)\n",
    "\n",
    "You will train two independent GPs, one to estimate HL and one to estimate CL. Each of the two GPs will be endowed with a composite kernel function $\\kappa_T = \\kappa_c \\cdot \\kappa_r + \\kappa_w$, where:\n",
    "- $\\kappa_c$ is a constant kernel\n",
    "- $\\kappa_r$ is an RBF kernel\n",
    "- $\\kappa_w$ is a White Noise kernel\n",
    "\n",
    "Evaluate those GPs with the corresponding test sets.\n",
    "\n",
    "**How do these results compare with those obtained using Random Forests?**\n",
    "\n",
    "**Discuss the contribution of the White Noise kernel and of the constant kernel to the final results**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DrepYjA80xz4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load composite kernel (GP, composite kernel): 0.5014581467097537\n",
      "RMSE for Cooling Load (GP, composite kernel): 1.7839732059330988\n",
      "Difference in % with respect to RF (Heating):  9.100005230889662e-05\n",
      "Difference in % with respect to RF (Cooling):  0.0003834009492530566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k1__constant_value is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, WhiteKernel, Sum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define composite kernel\n",
    "def composite_kernel():\n",
    "    rbf_kernel = RBF()\n",
    "    white_kernel = WhiteKernel()\n",
    "    return 1 * rbf_kernel + white_kernel\n",
    "\n",
    "# Define function to train GP and make predictions\n",
    "def train_and_predict_gp(X_train, y_train, X_test, kernel):\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, random_state=42\n",
    "                                  #, n_restarts_optimizer=10  #añadir al final\n",
    "                                 )\n",
    "    gp.fit(X_train, y_train)\n",
    "    y_pred, sigma = gp.predict(X_test, return_std=True)\n",
    "    return y_pred, sigma\n",
    "\n",
    "# Train and predict for Heating Load (HL)\n",
    "y_pred_heating, sigma_heating = train_and_predict_gp(X_train, y_train_heating.values.reshape(-1, 1), X_test, composite_kernel())\n",
    "rmse_heating_GP = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "\n",
    "# Train and predict for Cooling Load (CL)\n",
    "y_pred_cooling, sigma_cooling = train_and_predict_gp(X_train, y_train_cooling.values.reshape(-1, 1), X_test, composite_kernel())\n",
    "rmse_cooling_GP = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load composite kernel (GP, composite kernel):\", rmse_heating_GP)\n",
    "print(\"RMSE for Cooling Load (GP, composite kernel):\", rmse_cooling_GP)\n",
    "\n",
    "print(\"Difference in % with respect to RF (Heating): \", (rmse_heating_GP - rmse_heating_RF)/ 100)\n",
    "print(\"Difference in % with respect to RF (Cooling): \", (rmse_cooling_GP - rmse_cooling_RF)/ 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP, RBF + white): 11.039750612662663\n",
      "RMSE for Cooling Load (GP, RBF + white): 10.565696317123404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__length_scale is close to the specified upper bound 100000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## not using the constant kernel\n",
    "\n",
    "# Train and predict for Heating Load (HL)\n",
    "y_pred_heating, sigma_heating = train_and_predict_gp(X_train, y_train_heating.values.reshape(-1, 1), X_test, RBF() + WhiteKernel())\n",
    "rmse_heating = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "\n",
    "# Train and predict for Cooling Load (CL)\n",
    "y_pred_cooling, sigma_cooling = train_and_predict_gp(X_train, y_train_cooling.values.reshape(-1, 1), X_test,  RBF() + WhiteKernel())\n",
    "rmse_cooling = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP, RBF + white):\", rmse_heating)\n",
    "print(\"RMSE for Cooling Load (GP, RBF + white):\", rmse_cooling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP, RBF * constant): 25.08541117084569\n",
      "RMSE for Cooling Load (GP, RBF * constant): 27.0631746609165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## not using the white kernel\n",
    "# Train and predict for Heating Load (HL)\n",
    "y_pred_heating, sigma_heating = train_and_predict_gp(X_train, y_train_heating.values.reshape(-1, 1), X_test, 1 * RBF())\n",
    "rmse_heating = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "\n",
    "# Train and predict for Cooling Load (CL)\n",
    "y_pred_cooling, sigma_cooling = train_and_predict_gp(X_train, y_train_cooling.values.reshape(-1, 1), X_test, 1 * RBF())\n",
    "rmse_cooling = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP, RBF * constant):\", rmse_heating)\n",
    "print(\"RMSE for Cooling Load (GP, RBF * constant):\", rmse_cooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP, RBF): 1.8029858893509365\n",
      "RMSE for Cooling Load (GP, RBF): 2.77263105971425\n"
     ]
    }
   ],
   "source": [
    "## only using RBF\n",
    "y_pred_heating, sigma_heating = train_and_predict_gp(X_train, y_train_heating.values.reshape(-1, 1), X_test, RBF())\n",
    "rmse_heating = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "\n",
    "# Train and predict for Cooling Load (CL)\n",
    "y_pred_cooling, sigma_cooling = train_and_predict_gp(X_train, y_train_cooling.values.reshape(-1, 1), X_test, RBF())\n",
    "rmse_cooling = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP, RBF):\", rmse_heating)\n",
    "print(\"RMSE for Cooling Load (GP, RBF):\", rmse_cooling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experimenting with various values for the parameters of the Radial Basis Function (RBF) kernel, it was determined that the default parameters yielded the best performance metrics. Therefore, these default parameters were retained for subsequent analyses. \r\n",
    "\r\n",
    "Utilizing the composite kernel, which combines the RBF, constant, and white noise kernels, the results closely resembled those obtained from the Random Forest baseline, exhibiting less than 0.0001% difference in RMSE between the two approaches.\r\n",
    "\r\n",
    "To discern the individual contributions of the white noise and constant kernels, additional Gaussian Processes (GPs) were fitted without these kernels. The comparison underscored the importance of employing a composite kernel. Specifically, when omitting the constant kernel, the RMSE varied only slightly from the baseline results. However, the exclusion of the white noise kernel led to a significant increase in RMSE, particularly evident in the case of the cooling load, where the RMSE surged to 27, representing a deviation of more than 20 units from the basel e.\r\n",
    "\r\n",
    "In a final analysis, a GP solely utilizing the RBF kernel was tested. While the results were not as adverse as previous attempts, they still fell short of the performance achieved with the complete composite ke\n",
    "\n",
    "All of this can be explained because when using a composite kernel combining these components, the resulting GP model can capture various aspects of the data, including smooth trends (RBF), global offsets (constant), and noise (white noise). This composite kernel allows the GP to flexibly model complex patterns and uncertainties in the data, resulting in performance comparable to the Random Forest baseline.\r\n",
    "\r\n",
    "However, wheremovingve specific kernel components, such as the white noise kernel, you lose the ability to model certain aspects of the data. In the case of the white noise kernel, removing it can lead to a significant increase in prediction errors, especially when the data contains substantial noise or uncertainty. This is because without the white noise component, the GP may struggle to account for the inherent randomness or variability in the target variable, leading to poorer predictive performanc\n",
    "\n",
    "In summary, the composite kernel provides a holistic approach to modeling various aspects of the data, leading to comparable performance with the Random Forest baseline.e.vely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gaussian Process with Features selected by Random Forest (10%)\n",
    "\n",
    "Train now two independent GPs, using the same composite kernel as above, but now with those features indicated by RF as most relevant.\n",
    "\n",
    "**Is there any significant improvement over RF or GP with all the features?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP with selected features): 0.50168979277141\n",
      "RMSE for Cooling Load (GP with selected features): 1.7485613106096765\n",
      "Difference in % with respect to RF (Heating):  9.331651292545928e-05\n",
      "Difference in % with respect to RF (Cooling):  2.9281996018832857e-05\n",
      "Difference in % with respect to GP (Heating):  2.3164606165626544e-06\n",
      "Difference in % with respect to GP (Cooling):  -0.0003541189532342237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:445: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select most relevant features based on feature importances from Random Forest\n",
    "selected_features_heating = feature_importance_df.nlargest(5, 'Importance for Heating Load')['Feature'].values\n",
    "selected_features_cooling = feature_importance_df.nlargest(5, 'Importance for Cooling Load')['Feature'].values\n",
    "\n",
    "# Filter dataset to include only the selected features\n",
    "X_train_selected_heating = X_train[selected_features_heating]\n",
    "X_test_selected_heating = X_test[selected_features_heating]\n",
    "\n",
    "X_train_selected_cooling = X_train[selected_features_cooling]\n",
    "X_test_selected_cooling = X_test[selected_features_cooling]\n",
    "\n",
    "\n",
    "# Train and predict for Heating Load (HL) with selected features\n",
    "y_pred_heating_selected, sigma_heating_selected = train_and_predict_gp(X_train_selected_heating, y_train_heating.values.reshape(-1, 1), X_test_selected_heating, composite_kernel() )\n",
    "rmse_heating_selected = root_mean_squared_error(y_test_heating, y_pred_heating_selected)\n",
    "\n",
    "# Train and predict for Cooling Load (CL) with selected features\n",
    "y_pred_cooling_selected, sigma_cooling_selected = train_and_predict_gp(X_train_selected_cooling, y_train_cooling.values.reshape(-1, 1), X_test_selected_cooling,  composite_kernel())\n",
    "rmse_cooling_selected = root_mean_squared_error(y_test_cooling, y_pred_cooling_selected)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP with selected features):\", rmse_heating_selected)\n",
    "print(\"RMSE for Cooling Load (GP with selected features):\", rmse_cooling_selected)\n",
    "\n",
    "print(\"Difference in % with respect to RF (Heating): \", (rmse_heating_selected - rmse_heating_RF)/ 100)\n",
    "print(\"Difference in % with respect to RF (Cooling): \", (rmse_cooling_selected - rmse_cooling_RF)/ 100)\n",
    "\n",
    "print(\"Difference in % with respect to GP (Heating): \", (rmse_heating_selected - rmse_heating_GP)/ 100)\n",
    "print(\"Difference in % with respect to GP (Cooling): \", (rmse_cooling_selected - rmse_cooling_GP)/ 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult to determine which are the most important features because we cannot be sure how many do we need. After trying several values, finally we decided that the best results can be obtained by selecting the top 5 features. With that, when performing the GP with the composite kernel we can observe we have quite similar results that previously with RF or GP with all features (selecting less leads to much worst results). However, the RMSE is only improved in the case of ... (terminar de comentar cuando haga el final run porque varían los números).\n",
    "\n",
    "In general, there are no significant improvements in using the most relevant features only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Processes with ARD kernels (30%)\n",
    "\n",
    "Now use and ARD RBF kernel in the composite, that means enable a different lenghtscale for each feature. \n",
    "\n",
    "**Discuss the impact of the ARD kernel in the results of the GPs fit for each target.**\n",
    "\n",
    "**Print the parameters of the kernels after the GPs are fit. In particular discuss how the lengthscale achieved per each feature can be used as a proxy to estimate the relevance of each feature. Are these relevances aligned with those found by Random Forests?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP, ARD RBF): 0.48495202345235755\n",
      "RMSE for Cooling Load (GP, ARD RBF): 1.7301705932507432\n",
      "Kernel before fitting: 1**2 * RBF(length_scale=[1, 1, 1, 1, 1, 1, 1, 1]) + WhiteKernel(noise_level=1)\n",
      "Parameters of the ARD RBF kernel for Heating Load: 15.9**2 * RBF(length_scale=[1, 1, 1, 1, 1, 100, 0.234, 100]) + WhiteKernel(noise_level=0.189)\n",
      "\n",
      "Parameters of the ARD RBF kernel for Cooling Load: 18.2**2 * RBF(length_scale=[1, 1, 1, 1, 1, 100, 1.09, 100]) + WhiteKernel(noise_level=2.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit the GP models with ARD RBF kernel\n",
    "kernel_ard = 1.0 * RBF(length_scale=1*np.ones(8), length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()\n",
    "gp_heating_ard = GaussianProcessRegressor(kernel=kernel_ard, random_state=42)\n",
    "gp_cooling_ard = GaussianProcessRegressor(kernel=kernel_ard, random_state=42)\n",
    "\n",
    "gp_heating_ard.fit(X_train, y_train_heating)\n",
    "gp_cooling_ard.fit(X_train, y_train_cooling)\n",
    "\n",
    "y_pred_heating= gp_heating_ard.predict(X_test)\n",
    "y_pred_cooling= gp_cooling_ard.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_heating_ARD = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "rmse_cooling_ARD = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP, ARD RBF):\", rmse_heating_ARD)\n",
    "print(\"RMSE for Cooling Load (GP, ARD RBF):\", rmse_cooling_ARD)\n",
    "\n",
    "# Print parameters of the kernels\n",
    "print(\"Kernel before fitting:\", kernel_ard)\n",
    "print(\"Parameters of the ARD RBF kernel for Heating Load:\", gp_heating_ard.kernel_)\n",
    "print(\"\\nParameters of the ARD RBF kernel for Cooling Load:\", gp_cooling_ard.kernel_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss the impact of scaling or not the data in the conclusions extracted from the ARD results.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 4 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Heating Load (GP, ARD RBF): 0.49036084050424195\n",
      "RMSE for Cooling Load (GP, ARD RBF): 1.7205519185822173\n",
      "Kernel before fitting: 1**2 * RBF(length_scale=[1, 1, 1, 1, 1, 1, 1, 1]) + WhiteKernel(noise_level=1)\n",
      "Parameters of the ARD RBF kernel for Heating Load: 19.1**2 * RBF(length_scale=[0.871, 1.09, 2.32, 100, 100, 100, 2.28, 100]) + WhiteKernel(noise_level=0.193)\n",
      "\n",
      "Parameters of the ARD RBF kernel for Cooling Load: 17.8**2 * RBF(length_scale=[1.2, 0.604, 2.25, 100, 0.411, 100, 8.8, 100]) + WhiteKernel(noise_level=2.67)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 3 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 5 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\victoria\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:455: ConvergenceWarning: The optimal value found for dimension 7 of parameter k1__k2__length_scale is close to the specified upper bound 100.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the GP models with ARD RBF kernel\n",
    "kernel_ard = 1.0 * RBF(length_scale=1*np.ones(8), length_scale_bounds=(1e-2, 1e2)) + WhiteKernel()\n",
    "gp_heating_ard = GaussianProcessRegressor(kernel=kernel_ard, random_state=42)\n",
    "gp_cooling_ard = GaussianProcessRegressor(kernel=kernel_ard, random_state=42)\n",
    "\n",
    "gp_heating_ard.fit(X_train_scaled, y_train_heating)\n",
    "gp_cooling_ard.fit(X_train_scaled, y_train_cooling)\n",
    "\n",
    "# Predict using the fitted models\n",
    "y_pred_heating = gp_heating_ard.predict(X_test_scaled)\n",
    "y_pred_cooling = gp_cooling_ard.predict(X_test_scaled)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_heating_ARD = root_mean_squared_error(y_test_heating, y_pred_heating)\n",
    "rmse_cooling_ARD = root_mean_squared_error(y_test_cooling, y_pred_cooling)\n",
    "\n",
    "print(\"RMSE for Heating Load (GP, ARD RBF):\", rmse_heating_ARD)\n",
    "print(\"RMSE for Cooling Load (GP, ARD RBF):\", rmse_cooling_ARD)\n",
    "\n",
    "# Print parameters of the kernels\n",
    "print(\"Kernel before fitting:\", kernel_ard)\n",
    "print(\"Parameters of the ARD RBF kernel for Heating Load:\", gp_heating_ard.kernel_)\n",
    "print(\"\\nParameters of the ARD RBF kernel for Cooling Load:\", gp_cooling_ard.kernel_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predictive distributions (15%)\n",
    "\n",
    "The predictive distribution can be employed to asses the confidence on the predictions made by the GPs. For the GPs fit in parts 3 (composite kernel, single RBF lengthscale for all features) and 5 (ARD kernel) compute the predictions for the test data including mean and standard deviation.\n",
    "\n",
    "**For each GP produce an scatter plot of the absolute error of the predictions (true target minus mean of the predictive distributions) vs. the standard deviation of the corresponding predictive distribution. Is the standard deviation of the predictive distribution informative about the confidence in the predictions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced kernels (25%)\n",
    "\n",
    "Finally we will try to improve the results by using a more complicated kernel, which combine various covariance functions. Try to evaluate at least 10 different composite kernels by combining different instances of the basic kernels presented in class. Consider doing this programatically.\n",
    "\n",
    "**Discuss about how the diversity of the results in each target as you vary the composite kernels configuration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Homework_Gaussian Process.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
